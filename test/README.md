# cve-bin-tool tests

## Running all tests

To run the tests for `cve-bin-tool`

```console
python setup.py test
```

To run scanner and checker tests
```console
pytest test/test_scanner.py test/test_checkers.py
```

By default, some longer-running tests are turned off.  If you want to enable them, you can set the environment variable LONG_TESTS to 1.  You can do this just for a single command line as follows:

```console
LONG_TESTS=1 python setup.py test
```

For scanner tests
```console
LONG_TESTS=1 pytest test/test_scanner.py test/test_checkers.py
```

## Running a single test

To run a single test, you can use the unittest framework.  For example, here's how to run
the test for sqlite:

```console
python -m unittest test.test_scanner.TestScanner.test_sqlite_3_12_2
```

To run a single test in test_scanner you can use pytest. For example, here's how to run
the test for vendor_package_pairs:

```console
 pytest test/test_scanner.py::TestScanner::test_vendor_package_pairs
```

## Running tests on different versions of Python

Our CI currently runs tests on Python 3.6 and 3.7 under Linux.

The recommended way to do this yourself is to use python's `virtualenv`

You can set up virtualenv for all these environments:

```console
virtualenv -p python3.6 venv3.6
virtualenv -p python3.7 venv3.7
```

To activate one of these (the example uses 3.6), run the tests, and deactivate:

```console
source venv3.6/bin/activate
python setup.py test
deactivate

```

## Adding new tests: CVE mapping tests

* Existing tests are in [`test/`](https://github.com/intel/cve-bin-tool/tree/master/test)
* You can see the scanner tests in ['tests/test_scanner.py'](https://github.com/intel/cve-bin-tool/blob/master/test/test_scanner.py)
* To add a new one, make a new test case that detects a few CVEs known to occur in this version, and a few that are known not to occur in that version. For example, this is how the current tests look like. You should add the details of the new test case in the `@pytest.mark.parametrize` decorator of test_binaries test::

```python
    @pytest.mark.parametrize(
        "binary, package, version, are_in, not_in",
        [
            (
                "test-bluetoothctl-5.42libbluetooth.so.out",
                "bluetoothctl",
                "5.42",
                [
                    # for known CVE
                    "CVE-2016-9797",
                    "CVE-2016-9798",
                    "CVE-2016-9799",
                    "CVE-2016-9800",
                    "CVE-2016-9801",
                    "CVE-2016-9802",
                    "CVE-2016-9803",
                    "CVE-2016-9804",
                    "CVE-2016-9917",
                    # "CVE-2016-9918",
                ],
                [
                    # for older version
                    "CVE-2016-7837"
                ],
            ),
            (
                "test-icu-3.8.1.out",
                "international_components_for_unicode",
                "3.8.1",
                ["CVE-2007-4770", "CVE-2007-4771"],
                ["CVE-2019-3823"],
            ),
            (
                "test-icu-dos.out",
                "international_components_for_unicode",
                "3.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1",
                ["CVE-2014-9911"],
                ["CVE-2019-3823"],
            ),
            (
                "test-curl-7.34.0.out",
                "curl",
                "7.34.0",
                [
                    "CVE-2019-3823",
                    "CVE-2018-14618",
                    "CVE-2017-1000101",
                ],  # CVE-2017-1000101 needs supplemental data
                [],
            ),  
            .....
            .....
            .....
    def test_binaries(self, binary, package, version, are_in, not_in):
        self._binary_test(binary, package, version, are_in, not_in)
```

* Not sure what CVEs apply to a version? The checkers themselves often have links, and have the vendor/product pair so you can find them in the national vulnerability database as well. [Here's a link to the openssl checker for you to look at](https://github.com/intel/cve-bin-tool/blob/master/cve_bin_tool/checkers/openssl.py) and the rest are in [`cve-bin-tool/checkers/`](https://github.com/intel/cve-bin-tool/tree/master/cve_bin_tool/checkers)
* You'll also need to make a fake file designed to trick the checker into thinking it has found that version of the library.  You can see these files in [`test/binaries/`](https://github.com/intel/cve-bin-tool/tree/master/test/binaries)
* Please note that sometimes the database we're using doesn't have perfect mapping between CVEs and product versions -- if you try to write a test that doesn't work because of that mapping but the description in the CVE says that version should be vulnerable, don't discard it! Instead, please make a note of it in a github issue can investigate and maybe report it upstream.

## Adding new tests: Signature tests against real files

To make the basic test suite run quickly, we use "faked" binary files to test the **CVE mappings**.  However, we want to be able to test real files to test that the **signatures** work on real-world data.

In #99, I've added a _file_test function (to match the existing _binary_test) that takes a url, and package name and a version, and downloads the file, runs the scanner against it, and makes sure it is the file that you've specified.  But we need more tests!

* Existing tests are in [`test/`](https://github.com/intel/cve-bin-tool/tree/master/test)
* You can see the scanner tests in ['tests/test_scanner.py'](https://github.com/intel/cve-bin-tool/blob/master/test/test_scanner.py)
* To add a new test, find an appropriate publicly available file (linux distribution packages and public releases of the packages itself are ideal). You should add the details of the new test case in the `@pytest.mark.parametrize` decorator of test_files test
* Make sure to hide it behind the LONG_TESTS flag so we aren't doing huge number of downloads for every test suite run

```python
    @pytest.mark.parametrize(
        "url, filename, package, version",
        list(
            itertools.chain(
                [
                    (
                        "https://archives.fedoraproject.org/pub/archive/fedora/linux"
                        "/releases/20/Everything/x86_64/os/Packages/c/",
                        "curl-7.32.0-3.fc20.x86_64.rpm",
                        "curl",
                        "7.32.0",
                    ),
                    (
                        "http://mirror.centos.org/centos/7/os/x86_64/Packages/",
                        "expat-2.1.0-10.el7_3.i686.rpm",
                        "expat",
                        "2.1.0",
                    ),
                    (
                        "http://http.us.debian.org/debian/pool/main/e/expat/",
                        "libexpat1_2.2.0-2+deb9u3_amd64.deb",
                        "expat",
                        "2.2.0",
                    ),
                    (
                        "http://archive.ubuntu.com/ubuntu/pool/universe/f/ffmpeg/",
                        "ffmpeg_4.1.1-1_amd64.deb",
                        "ffmpeg",
                        "4.1.1",
                    ),
                    .....
                    .....
                    .....
    @unittest.skipUnless(LONG_TESTS() > 0, "Skipping long tests")
    def test_files(self, url, filename, package, version):
        self._file_test(url, filename, package, version) 
```

Ideally, we should have at least one such test for each checker, and it would be nice to have some different sources for each as well. For example, for packages available in common Linux distributions, we might want to have one from fedora, one from debian, and one direct from upstream to show that we detect all those versions.

Note that we're getting the LONG_TESTS() from tests.util in the top of the files where it's being used.  If you're adding a long test to a test file that previously didn't have any, you'll need to add that at the top of the file as well.

## Adding new tests: Checker filename mappings

To test the filename mappings, rather than making a bunch of empty files, we're calling the checkers directly in `test/test_checkers.py`.  You can add a new test by specifying a the name of the checker you want to test, the file name, and the expected result that the scanner should say it "is".

```python
    @pytest.mark.parametrize(
        "checker_name, file_name, expected_result",
        [
            ("bluez", "libbluetooth.so.4", "bluetoothctl"),
            ("python", "python", "python"),
            ("python", "python3.8", "python"),
        ],
    )
```

The function test_filename_is will then load the checker you have specified (and fail spectacularly if you specify a checker that does not exist), try to run get_version() with an empty file content and the filename you specified, then check that it "is" something (as opposed to "contains") and that the modulename that get_version returns is in fact the expected_result you specified.

For ease of maintenance, please keep the parametrize list in alphabetical order when you add a new tests.

You can then run your new test using pytest:
```console
pytest test/test_checker.py
```

You can also use all the pytest functionality to run groups of tests.  For example, this will run the python-related tests (but not the bluetooth one):

```console
pytest -v test/test_checkers.py -k python
```
