"""
Retrieval access and caching of NIST CVE database
"""
import os
import re
import gzip
import json
import glob
import shutil
import hashlib
import logging
import tempfile
import functools
import traceback
import contextlib
import multiprocessing

try:
    import urllib.request as request
except:
    import urllib2 as request

from .log import LOGGER

logging.basicConfig(level=logging.DEBUG)


class EmptyCache(Exception):
    """
    Raised when NVD is opened when verify=False and there are no files in the
    cache.
    """


class CVEDataForYearNotInCache(Exception):
    """
    Raised when the CVE data for a year is not present in the cache.
    """


class AttemptedToWriteOutsideCachedir(Exception):
    """
    Raised if we attempted to write to a file that would have been outside the
    cachedir.
    """


class SHAMismatch(Exception):
    """
    Raised if the sha of a file in the cache was not what it should be.
    """


def log_traceback(func, *args, **kwargs):
    """
    Multiprocessing won't print tracebacks, so log them
    """
    logger = logging.getLogger(__name__ + "." + func.__name__)
    try:
        return func(*args, logger=logger, **kwargs)
    except:
        logger.error(traceback.format_exc().strip())
        raise


def getmeta(metaurl, logger=LOGGER):
    with contextlib.closing(request.urlopen(metaurl)) as response:
        return (
            metaurl.replace(".meta", ".json.gz"),
            dict(
                [
                    line.split(":", 1)
                    for line in (response.read().decode()).split("\r\n")
                    if ":" in line
                ]
            ),
        )


def cache_update(cachedir, url, sha, chunk_size=16 * 1024, logger=LOGGER):
    """
    Update the cache for a single year of NVD data.
    """
    filename = url.split("/")[-1].replace(".gz", "")
    # Ensure we only write to files within the cachedir
    filepath = os.path.abspath(os.path.join(cachedir, filename))
    if not filepath.startswith(os.path.abspath(cachedir)):
        raise AttemptedToWriteOutsideCachedir(filepath)
    # Validate the contents of the cached file
    if os.path.isfile(filepath):
        # Validate the sha and write out
        sha = sha.upper()
        calculate = hashlib.sha256()
        with open(filepath, "rb") as handle:
            chunk = handle.read(chunk_size)
            while chunk:
                calculate.update(chunk)
                chunk = handle.read(chunk_size)
        # Validate the sha and exit if it is correct, otherwise update
        gotsha = calculate.hexdigest().upper()
        if gotsha != sha:
            os.unlink(filepath)
            logger.critical(
                "SHA mismatch for %s (have: %r, want: %r)", filename, gotsha, sha
            )
        else:
            logger.debug("Correct SHA for %s", filename)
            return
    logger.info("Updating CVE cache for %s", filename)
    with tempfile.TemporaryFile(prefix="cvedb-") as temp_file:
        with contextlib.closing(request.urlopen(url)) as response:
            # Write to tempfile (gzip doesnt support reading from urlopen on
            # Python 2)
            shutil.copyfileobj(response, temp_file)
        # Replace the file with the tempfile
        temp_file.seek(0)
        with gzip.GzipFile(fileobj=temp_file, mode="rb") as jsondata_fileobj:
            # Validate the sha
            sha = sha.upper()
            calculate = hashlib.sha256()
            # Copy the contents while updating the sha
            with open(filepath, "wb") as filepath_handle:
                chunk = jsondata_fileobj.read(chunk_size)
                while chunk:
                    calculate.update(chunk)
                    filepath_handle.write(chunk)
                    chunk = jsondata_fileobj.read(chunk_size)
            # Raise error if there was an issue with the sha
            gotsha = calculate.hexdigest().upper()
            if gotsha != sha:
                # Remove the file if there was an issue
                os.unlink(filepath)
                raise SHAMismatch("{} (have: {}, want: {})".format(url, gotsha, sha))


class CVEDB(object):
    """
    Downloads NVD data in json form and stores it on disk in a cache.
    """

    CACHEDIR = os.path.join(os.path.expanduser("~"), ".cache", "cvedb")
    FEED = "https://nvd.nist.gov/vuln/data-feeds"
    LOGGER = LOGGER.getChild("CVEDB")
    NVDCVE_FILENAME_TEMPLATE = "nvdcve-1.1-{}.json"
    META_REGEX = re.compile("https:\/\/.*\/json\/.*-[0-9]*\.[0-9]*-[0-9]*\.meta")

    def __init__(self, verify=True, feed=None, cachedir=None):
        self.verify = verify
        self.feed = feed if feed is not None else self.FEED
        self.cachedir = cachedir if cachedir is not None else self.CACHEDIR
        # Will be true if refresh was successful
        self.was_updated = False

    def nist_scrape(self, feed):
        with contextlib.closing(request.urlopen(feed)) as response:
            page = response.read().decode()
            jsonmetalinks = self.META_REGEX.findall(page)
            pool = multiprocessing.Pool()
            try:
                metadata = dict(
                    pool.map(
                        functools.partial(log_traceback, getmeta), tuple(jsonmetalinks)
                    )
                )
                pool.close()
                return metadata
            except:
                pool.terminate()
                raise
            finally:
                pool.join()

    def parse(self):
        # eventually: for year in years()
        cve_data = self.year(2019)
        for cve_item in cve_data["CVE_Items"]:
            self.LOGGER.debug(cve_item["cve"]["CVE_data_meta"]["ID"])
            if "baseMetricV3" in cve_item["impact"]:
                self.LOGGER.debug(
                    cve_item["impact"]["baseMetricV3"]["cvssV3"]["baseSeverity"]
                )

    #            else if "baseMetricV2" in cve_item["impact"]:

    def refresh(self):
        if not os.path.isdir(self.cachedir):
            os.makedirs(self.cachedir)
        update = self.nist_scrape(self.feed)
        pool = multiprocessing.Pool()
        try:
            for result in [
                pool.apply_async(
                    functools.partial(log_traceback, cache_update),
                    (self.cachedir, url, meta["sha256"]),
                )
                for url, meta in update.items()
            ]:
                result.get()
            pool.close()
            self.was_updated = True
        except:
            pool.terminate()
            raise
        finally:
            pool.join()

    def year(self, year):
        """
        Return the dict of CVE data for the given year.
        """
        filename = os.path.join(
            self.cachedir, self.NVDCVE_FILENAME_TEMPLATE.format(year)
        )
        # Check if file exists
        if not os.path.isfile(filename):
            raise CVEDataForYearNotInCache(year)
        # Open the file and load the JSON data, log the number of CVEs loaded
        with open(filename, "rb") as fileobj:
            cves_for_year = json.load(fileobj)
            self.LOGGER.debug(
                "Year %d has %d CVEs in dataset", year, len(cves_for_year["CVE_Items"])
            )
            return cves_for_year

    def years(self):
        """
        Return the years we have NVD data for.
        """
        return sorted(
            [
                int(filename.split(".")[-2].split("-")[-1])
                for filename in glob.glob(
                    os.path.join(self.cachedir, "nvdcve-1.1-*.json")
                )
            ]
        )

    def __enter__(self):
        if not self.verify:
            self.LOGGER.error("Not verifying CVE DB cache")
            if not self.years():
                raise EmptyCache(self.cachedir)
        else:
            self.refresh()
        self.LOGGER.debug("Years present: %s", self.years())
        return self

    def __exit__(self, _exc_type, _exc_value, _traceback):
        pass


def refresh():
    with CVEDB():
        pass


if __name__ == "__main__":
    print("Experimenting...")
    cvedb = CVEDB(os.path.join(os.path.expanduser("~"), ".cache", "cvedb"))
    cvedb.refresh()
    print(cvedb.years())
    cvedb.parse()
