# Copyright (C) 2022 Intel Corporation
# SPDX-License-Identifier: GPL-3.0-or-later

import asyncio
import datetime
import io
import json
import zipfile
from pathlib import Path

import aiohttp

from cve_bin_tool.async_utils import FileIO, aio_run_command
from cve_bin_tool.data_sources import DISK_LOCATION_DEFAULT, Data_Source
from cve_bin_tool.error_handler import ErrorMode
from cve_bin_tool.log import LOGGER


class OSV_Source(Data_Source):

    CACHEDIR = DISK_LOCATION_DEFAULT
    LOGGER = LOGGER.getChild("CVEDB")
    OSV_URL = "https://osv-vulnerabilities.storage.googleapis.com/"
    OSV_GS_URL = "gs://osv-vulnerabilities/"

    def __init__(
        self, error_mode: ErrorMode = ErrorMode.TruncTrace, incremental_update=False
    ):
        self.cachedir = self.CACHEDIR
        self.ecosystems = None
        self.osv_path = str(Path(self.cachedir) / "osv")

        self.error_mode = error_mode
        self.incremental_update = incremental_update

        self.osv_url = self.OSV_URL
        self.gs_url = self.OSV_GS_URL
        self.all_cve_entries = []

    async def update_ecosystems(self):
        ecosystems = []

        stdout, stderr, _ = await aio_run_command(["gsutil", "ls", self.gs_url])
        stdout = str(stdout).split("gs")
        stdout.pop(0)

        for line in stdout:
            ecosystem = line.split("/")[-2]
            ecosystems.append(ecosystem)

        self.ecosystems = ecosystems

    async def get_ecosystem(self, ecosystem_url, mode="json"):
        async with aiohttp.ClientSession() as session:
            async with session.get(ecosystem_url) as r:
                if mode == "bytes":
                    content = await r.read()
                else:
                    content = await r.json()
                return content

    async def get_ecosystem_incremental(self, ecosystem, time_of_last_update):
        gs_file = self.gs_url + ecosystem
        stdout, stderr, _ = await aio_run_command(["gsutil", "ls", "-l", gs_file])
        stdout = str(stdout).split("json")

        newfiles = []

        for line in stdout:
            filename, timestamp = self.parse_filename(line)

            if timestamp is not None and timestamp > time_of_last_update:
                newfiles.append(filename)

        tasks = []

        for file in newfiles:
            eco_url = self.osv_url + ecosystem + "/" + file
            task = self.get_ecosystem(eco_url)
            tasks.append(task)

        for r in await asyncio.gather(*tasks):
            filepath = Path(self.osv_path) / (r.get("id") + ".json")
            r = json.dumps(r)

            async with FileIO(filepath, "w") as f:
                await f.write(r)

    def parse_filename(self, str):
        str = str.split("  ")

        filename = str[-1]

        if "zip" in filename:
            return None, None

        filename = filename.split("/")[-1] + "json"
        timestamp = datetime.datetime.strptime(str[-2], "%Y-%m-%dT%H:%M:%SZ")

        return filename, timestamp

    async def fetch_cves(self):
        LOGGER.info("Getting OSV CVEs...")

        from cve_bin_tool import cvedb  # prevent cyclic import

        db = cvedb.CVEDB()

        if self.incremental_update:
            time_of_last_update = datetime.datetime.fromtimestamp(
                db.get_db_update_date()
            )
            for ecosystem in self.ecosystems:
                await self.get_ecosystem_incremental(ecosystem, time_of_last_update)
        else:
            tasks = []

            for ecosystem in self.ecosystems:
                eco_url = self.osv_url + ecosystem + "/all.zip"
                task = self.get_ecosystem(eco_url, mode="bytes")

                tasks.append(task)

            for r in await asyncio.gather(*tasks):

                z = zipfile.ZipFile(io.BytesIO(r))
                z.extractall(self.osv_path)

    async def update_cve_entries(self):
        p = Path(self.osv_path).glob("**/*")
        files = [x for x in p if x.is_file()]

        for file in files:
            async with FileIO(file, "r") as f:
                r = await f.read()
                data = json.loads(r)

                self.all_cve_entries.append(data)

    def format_data(self, all_cve_entries):
        severity_data = []
        affected_data = []

        for cve_item in all_cve_entries:
            cve_id = (
                cve_item.get("aliases")[0]
                if cve_item.get("aliases", None) is not None
                and "CVE" in cve_item.get("aliases")[0]
                else cve_item["id"]
            )

            cve = {
                "ID": cve_id,
                "severity": None,
                "description": cve_item.get("summary", None),
                "score": None,
                "CVSS_version": None,
                "CVSS_vector": None,
            }

            severity_data.append(cve)

            affected = {
                "cve_id": cve_id,
                "vendor": cve_item["affected"][0]["package"]["ecosystem"],
                "product": cve_item["affected"][0]["package"]["name"].split("/")[-1],
                "version": None,
                "versionStartIncluding": None,
                "versionStartExcluding": None,
                "versionEndIncluding": None,
                "versionEndExcluding": None,
            }

            affected_data.append(affected)

        return severity_data, affected_data

    async def get_cve_data(self):
        await self.update_ecosystems()
        await self.fetch_cves()
        await self.update_cve_entries()

        return self.format_data(self.all_cve_entries)
